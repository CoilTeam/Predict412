<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>1. Introduction</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<p><strong>Setup</strong></p>

<pre><code class="r">library(kernlab)
# library(car)
library(ggplot2)
library(corrgram)
</code></pre>

<pre><code>## Warning: package &#39;corrgram&#39; was built under R version 3.0.3
</code></pre>

<pre><code>## Loading required package: seriation
</code></pre>

<pre><code>## Warning: package &#39;seriation&#39; was built under R version 3.0.3
</code></pre>

<pre><code class="r">library(vcd)
</code></pre>

<pre><code>## Warning: package &#39;vcd&#39; was built under R version 3.0.3
</code></pre>

<pre><code>## Loading required package: grid
</code></pre>

<pre><code class="r">library(rpart)
library(randomForest)
</code></pre>

<pre><code>## Warning: package &#39;randomForest&#39; was built under R version 3.0.3
</code></pre>

<pre><code>## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<pre><code class="r">library(e1071)
</code></pre>

<pre><code>## Warning: package &#39;e1071&#39; was built under R version 3.0.3
</code></pre>

<pre><code class="r">library(nnet)
</code></pre>

<pre><code>## Warning: package &#39;nnet&#39; was built under R version 3.0.3
</code></pre>

<h1>1. Introduction</h1>

<h1>2. Process</h1>

<h2>2.1 Data Preparation</h2>

<h3>2.1.1 Import Dataset</h3>

<pre><code class="r">data(ticdata)
# raw &lt;- read.csv(&#39;../ticdata2000.csv&#39;)
data &lt;- ticdata[1:5822, ]
</code></pre>

<h3>2.1.2 Modify Datatypes</h3>

<pre><code class="r"># categorical_names &lt;- grep(&#39;^m|^caravan&#39;, names(data)) # not needed for
# kernlab data[,categorical_names] &lt;- lapply(data[,categorical_names],
# as.factor)
dc &lt;- as.data.frame(lapply(data, as.factor))  # transform all to categorical

str(dc)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    5822 obs. of  86 variables:
##  $ STYPE   : Factor w/ 39 levels &quot;Affluent senior apartments&quot;,..: 15 20 20 21 11 36 13 15 15 7 ...
##  $ MAANTHUI: Factor w/ 9 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 2 1 1 2 ...
##  $ MGEMOMV : Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 3 2 2 3 4 2 3 2 2 3 ...
##  $ MGEMLEEF: Factor w/ 6 levels &quot;20-30&quot;,&quot;30-40&quot;,..: 2 2 2 3 2 1 2 3 4 3 ...
##  $ MOSHOOFD: Factor w/ 10 levels &quot;Average Family&quot;,..: 6 6 6 1 7 8 3 6 6 1 ...
##  $ MGODRK  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 2 1 3 2 1 3 1 1 4 ...
##  $ MGODPR  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 6 5 5 4 5 6 3 8 2 6 ...
##  $ MGODOV  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 2 3 3 2 1 1 1 4 1 ...
##  $ MGODGE  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 4 5 5 5 5 6 6 3 7 3 ...
##  $ MRELGE  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 8 7 4 6 8 1 8 8 7 8 ...
##  $ MRELSA  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 3 3 3 2 7 3 3 1 1 ...
##  $ MRELOV  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 3 5 3 3 4 1 1 4 3 ...
##  $ MFALLEEN: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 5 3 3 4 1 1 4 3 ...
##  $ MFGEKIND: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 5 5 4 5 6 4 6 4 3 ...
##  $ MFWEKIND: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 7 6 3 5 5 3 7 5 4 7 ...
##  $ MOPLHOOG: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 1 4 6 1 1 1 1 1 ...
##  $ MOPLMIDD: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 6 6 5 5 6 5 4 2 5 ...
##  $ MOPLLAAG: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 8 5 5 3 1 5 6 7 9 6 ...
##  $ MBERHOOG: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 1 5 1 3 1 3 2 3 ...
##  $ MBERZELF: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 1 1 1 6 1 1 1 2 1 ...
##  $ MBERBOER: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 1 1 5 1 1 1 1 1 ...
##  $ MBERMIDD: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 6 8 4 1 5 5 3 2 4 ...
##  $ MBERARBG: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 6 1 1 2 1 3 2 6 9 4 ...
##  $ MBERARBO: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 5 3 3 1 3 6 3 2 4 ...
##  $ MSKA    : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 1 4 10 3 1 3 2 2 ...
##  $ MSKB1   : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 3 6 3 1 3 2 2 2 3 ...
##  $ MSKB2   : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 3 4 1 2 1 3 5 3 1 2 ...
##  $ MSKC    : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 7 6 5 5 1 5 6 6 9 5 ...
##  $ MSKD    : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 1 1 1 1 3 1 3 2 3 ...
##  $ MHHUUR  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 3 8 6 5 10 7 1 10 1 ...
##  $ MHKOOP  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 9 8 3 5 6 1 4 10 1 10 ...
##  $ MAUT1   : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 9 8 8 10 7 6 9 5 6 7 ...
##  $ MAUT2   : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 2 1 1 3 4 1 5 3 2 ...
##  $ MAUT0   : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 3 3 1 2 4 2 3 4 3 ...
##  $ MZFONDS : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 9 7 10 8 6 10 10 7 8 7 ...
##  $ MZPART  : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 2 4 1 3 5 1 1 4 3 4 ...
##  $ MINKM30 : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 3 5 2 1 6 5 3 8 3 ...
##  $ MINK3045: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 5 1 6 6 1 3 4 6 3 4 ...
##  $ MINK4575: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 6 6 1 4 10 4 4 4 2 4 ...
##  $ MINK7512: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 3 1 1 1 1 1 1 1 2 ...
##  $ MINK123M: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ MINKGEM : Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 5 6 4 5 7 4 4 4 3 5 ...
##  $ MKOOPKLA: Ord.factor w/ 10 levels &quot;0%&quot;&lt;&quot;1 - 10%&quot;&lt;..: 4 5 5 5 4 4 6 4 4 8 ...
##  $ PWAPART : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 3 3 1 1 1 1 1 1 3 ...
##  $ PWABEDR : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PWALAND : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PPERSAUT: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 7 1 7 7 1 7 7 1 6 1 ...
##  $ PBESAUT : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PMOTSCO : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AAUT    : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PAANHANG: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PTRACTOR: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PWERKT  : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PBROM   : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 4 1 1 ...
##  $ PLEVEN  : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PPERSONG: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PGEZONG : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PWAOREG : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PBRAND  : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 6 3 3 3 7 1 1 1 1 4 ...
##  $ PZEILPL : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PPLEZIER: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PFIETS  : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PINBOED : Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ PBYSTAND: Ord.factor w/ 10 levels &quot;f 0&quot;&lt;&quot;f 1-49&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AWAPART : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 3 2 1 1 1 1 1 1 2 ...
##  $ AWABEDR : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;5&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AWALAND : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ APERSAUT: Factor w/ 7 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 2 1 2 2 1 2 2 1 2 1 ...
##  $ ABESAUT : Factor w/ 5 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AMOTSCO : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;8&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AVRAAUT : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AAANHANG: Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ATRACTOR: Factor w/ 5 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AWERKT  : Factor w/ 5 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ABROM   : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 2 1 1 ...
##  $ ALEVEN  : Factor w/ 6 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ APERSONG: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AGEZONG : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AWAOREG : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ABRAND  : Factor w/ 7 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 2 2 2 2 2 1 1 1 1 2 ...
##  $ AZEILPL : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ APLEZIER: Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AFIETS  : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ AINBOED : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ABYSTAND: Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ CARAVAN : Factor w/ 2 levels &quot;noinsurance&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<pre><code class="r">head(dc)
</code></pre>

<pre><code>##                        STYPE MAANTHUI MGEMOMV MGEMLEEF
## 1 Lower class large families        1       3    30-40
## 2  Mixed small town dwellers        1       2    30-40
## 3  Mixed small town dwellers        1       2    30-40
## 4  Modern, complete families        1       3    40-50
## 5         Large family farms        1       4    30-40
## 6           Young and rising        1       2    20-30
##                MOSHOOFD   MGODRK   MGODPR   MGODOV   MGODGE   MRELGE
## 1 Family with grown ups       0% 50 - 62%  1 - 10% 24 - 36% 76 - 88%
## 2 Family with grown ups  1 - 10% 37 - 49%  1 - 10% 37 - 49% 63 - 75%
## 3 Family with grown ups       0% 37 - 49% 11 - 23% 37 - 49% 24 - 36%
## 4        Average Family 11 - 23% 24 - 36% 11 - 23% 37 - 49% 50 - 62%
## 5               Farmers  1 - 10% 37 - 49%  1 - 10% 37 - 49% 76 - 88%
## 6           Living well       0% 50 - 62%       0% 50 - 62%       0%
##     MRELSA   MRELOV MFALLEEN MFGEKIND MFWEKIND MOPLHOOG MOPLMIDD MOPLLAAG
## 1       0% 11 - 23%  1 - 10% 11 - 23% 63 - 75%  1 - 10% 11 - 23% 76 - 88%
## 2 11 - 23% 11 - 23%       0% 37 - 49% 50 - 62%       0% 50 - 62% 37 - 49%
## 3 11 - 23% 37 - 49% 37 - 49% 37 - 49% 11 - 23%       0% 50 - 62% 37 - 49%
## 4 11 - 23% 11 - 23% 11 - 23% 24 - 36% 37 - 49% 24 - 36% 37 - 49% 11 - 23%
## 5  1 - 10% 11 - 23% 11 - 23% 37 - 49% 37 - 49% 50 - 62% 37 - 49%       0%
## 6 63 - 75% 24 - 36% 24 - 36% 50 - 62% 11 - 23%       0% 50 - 62% 37 - 49%
##   MBERHOOG MBERZELF MBERBOER MBERMIDD MBERARBG MBERARBO     MSKA    MSKB1
## 1  1 - 10%       0%  1 - 10% 11 - 23% 50 - 62% 11 - 23%  1 - 10%  1 - 10%
## 2       0%       0%       0% 50 - 62%       0% 37 - 49%       0% 11 - 23%
## 3       0%       0%       0% 76 - 88%       0% 11 - 23%       0% 50 - 62%
## 4 37 - 49%       0%       0% 24 - 36%  1 - 10% 11 - 23% 24 - 36% 11 - 23%
## 5       0% 50 - 62% 37 - 49%       0%       0%       0%     100%       0%
## 6 11 - 23%       0%       0% 37 - 49% 11 - 23% 11 - 23% 11 - 23% 11 - 23%
##      MSKB2     MSKC     MSKD   MHHUUR   MHKOOP    MAUT1    MAUT2    MAUT0
## 1 11 - 23% 63 - 75%  1 - 10%  1 - 10% 89 - 99% 89 - 99%       0%  1 - 10%
## 2 24 - 36% 50 - 62%       0% 11 - 23% 76 - 88% 76 - 88%  1 - 10% 11 - 23%
## 3       0% 37 - 49%       0% 76 - 88% 11 - 23% 76 - 88%       0% 11 - 23%
## 4  1 - 10% 37 - 49%       0% 50 - 62% 37 - 49%     100%       0%       0%
## 5       0%       0%       0% 37 - 49% 50 - 62% 63 - 75% 11 - 23%  1 - 10%
## 6 11 - 23% 37 - 49% 11 - 23%     100%       0% 50 - 62% 24 - 36% 24 - 36%
##    MZFONDS   MZPART  MINKM30 MINK3045 MINK4575 MINK7512 MINK123M  MINKGEM
## 1 89 - 99%  1 - 10%       0% 37 - 49% 50 - 62%       0%       0% 37 - 49%
## 2 63 - 75% 24 - 36% 11 - 23%       0% 50 - 62% 11 - 23%       0% 50 - 62%
## 3     100%       0% 37 - 49% 50 - 62%       0%       0%       0% 24 - 36%
## 4 76 - 88% 11 - 23%  1 - 10% 50 - 62% 24 - 36%       0%       0% 37 - 49%
## 5 50 - 62% 37 - 49%       0%       0%     100%       0%       0% 63 - 75%
## 6     100%       0% 50 - 62% 11 - 23% 24 - 36%       0%       0% 24 - 36%
##   MKOOPKLA PWAPART PWABEDR PWALAND    PPERSAUT PBESAUT PMOTSCO AAUT
## 1 24 - 36%     f 0     f 0     f 0 f 1000-4999     f 0     f 0  f 0
## 2 37 - 49% f 50-99     f 0     f 0         f 0     f 0     f 0  f 0
## 3 37 - 49% f 50-99     f 0     f 0 f 1000-4999     f 0     f 0  f 0
## 4 37 - 49%     f 0     f 0     f 0 f 1000-4999     f 0     f 0  f 0
## 5 24 - 36%     f 0     f 0     f 0         f 0     f 0     f 0  f 0
## 6 24 - 36%     f 0     f 0     f 0 f 1000-4999     f 0     f 0  f 0
##   PAANHANG PTRACTOR PWERKT PBROM PLEVEN PPERSONG PGEZONG PWAOREG
## 1      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
## 2      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
## 3      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
## 4      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
## 5      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
## 6      f 0      f 0    f 0   f 0    f 0      f 0     f 0     f 0
##        PBRAND PZEILPL PPLEZIER PFIETS PINBOED PBYSTAND AWAPART AWABEDR
## 1   f 500-999     f 0      f 0    f 0     f 0      f 0       0       0
## 2     f 50-99     f 0      f 0    f 0     f 0      f 0       2       0
## 3     f 50-99     f 0      f 0    f 0     f 0      f 0       1       0
## 4     f 50-99     f 0      f 0    f 0     f 0      f 0       0       0
## 5 f 1000-4999     f 0      f 0    f 0     f 0      f 0       0       0
## 6         f 0     f 0      f 0    f 0     f 0      f 0       0       0
##   AWALAND APERSAUT ABESAUT AMOTSCO AVRAAUT AAANHANG ATRACTOR AWERKT ABROM
## 1       0        1       0       0       0        0        0      0     0
## 2       0        0       0       0       0        0        0      0     0
## 3       0        1       0       0       0        0        0      0     0
## 4       0        1       0       0       0        0        0      0     0
## 5       0        0       0       0       0        0        0      0     0
## 6       0        1       0       0       0        0        0      0     0
##   ALEVEN APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED
## 1      0        0       0       0      1       0        0      0       0
## 2      0        0       0       0      1       0        0      0       0
## 3      0        0       0       0      1       0        0      0       0
## 4      0        0       0       0      1       0        0      0       0
## 5      0        0       0       0      1       0        0      0       0
## 6      0        0       0       0      0       0        0      0       0
##   ABYSTAND     CARAVAN
## 1        0 noinsurance
## 2        0 noinsurance
## 3        0 noinsurance
## 4        0 noinsurance
## 5        0 noinsurance
## 6        0 noinsurance
</code></pre>

<h2>2.2 Data Exploration</h2>

<h3>2.2.1 Summary EDA</h3>

<pre><code class="r">summary(dc)
</code></pre>

<pre><code>##                         STYPE         MAANTHUI    MGEMOMV   MGEMLEEF   
##  Lower class large families: 810   1      :5267   1: 284   20-30:  74  
##  Middle class families     : 339   2      : 505   2:2131   30-40:1452  
##  Traditional families      : 339   3      :  39   3:2646   40-50:3000  
##  Large religous families   : 328   7      :   5   4: 693   50-60:1073  
##  Modern, complete families : 278   4      :   2   5:  68   60-70: 193  
##  Young and rising          : 251   5      :   1            70-80:  30  
##  (Other)                   :3477   (Other):   3                        
##                   MOSHOOFD         MGODRK          MGODPR    
##  Family with grown ups:1563   0%      :3228   37 - 49%:1607  
##  Average Family       : 886   1 - 10% :1599   50 - 62%:1501  
##  Conservative families: 667   11 - 23%: 733   63 - 75%: 714  
##  Living well          : 569   24 - 36%: 152   24 - 36%: 590  
##  Successful hedonists : 552   37 - 49%:  66   76 - 88%: 564  
##  Retired and Religeous: 550   50 - 62%:  18   11 - 23%: 396  
##  (Other)              :1035   (Other) :  26   (Other) : 450  
##       MGODOV          MGODGE          MRELGE          MRELSA    
##  1 - 10% :2014   24 - 36%:1453   76 - 88%:1683   0%      :2448  
##  0%      :2003   37 - 49%:1334   63 - 75%:1172   1 - 10% :2030  
##  11 - 23%:1388   11 - 23%:1055   50 - 62%: 946   11 - 23%:1075  
##  24 - 36%: 257   50 - 62%: 963   100%    : 794   24 - 36%: 159  
##  37 - 49%: 132   0%      : 456   89 - 99%: 361   37 - 49%:  78  
##  50 - 62%:  28   1 - 10% : 230   37 - 49%: 324   50 - 62%:  18  
##  (Other) :   0   (Other) : 331   (Other) : 542   (Other) :  14  
##       MRELOV         MFALLEEN        MFGEKIND        MFWEKIND   
##  11 - 23%:1756   0%      :1757   24 - 36%:1498   37 - 49%:1137  
##  0%      :1173   11 - 23%:1247   37 - 49%:1455   50 - 62%:1106  
##  24 - 36%:1152   1 - 10% : 951   11 - 23%:1060   24 - 36%: 973  
##  37 - 49%: 648   24 - 36%: 848   50 - 62%: 606   63 - 75%: 783  
##  1 - 10% : 539   37 - 49%: 519   1 - 10% : 372   11 - 23%: 635  
##  50 - 62%: 266   50 - 62%: 259   0%      : 371   76 - 88%: 351  
##  (Other) : 288   (Other) : 241   (Other) : 460   (Other) : 837  
##      MOPLHOOG        MOPLMIDD        MOPLLAAG        MBERHOOG   
##  0%      :2147   37 - 49%:1426   50 - 62%:1009   0%      :1524  
##  1 - 10% :1322   24 - 36%:1330   63 - 75%: 856   11 - 23%:1364  
##  11 - 23%:1144   11 - 23%: 937   37 - 49%: 851   1 - 10% :1245  
##  24 - 36%: 547   50 - 62%: 738   24 - 36%: 680   24 - 36%: 756  
##  37 - 49%: 326   0%      : 423   11 - 23%: 667   37 - 49%: 397  
##  50 - 62%: 187   1 - 10% : 383   76 - 88%: 640   50 - 62%: 249  
##  (Other) : 149   (Other) : 585   (Other) :1119   (Other) : 287  
##      MBERZELF        MBERBOER        MBERMIDD        MBERARBG   
##  0%      :4171   0%      :4176   11 - 23%:1491   11 - 23%:1382  
##  1 - 10% :1202   1 - 10% : 854   24 - 36%:1394   0%      :1167  
##  11 - 23%: 348   11 - 23%: 487   37 - 49%: 953   24 - 36%:1167  
##  50 - 62%:  52   24 - 36%: 143   0%      : 667   1 - 10% : 921  
##  24 - 36%:  37   37 - 49%:  77   50 - 62%: 431   37 - 49%: 604  
##  37 - 49%:  12   50 - 62%:  59   1 - 10% : 403   50 - 62%: 310  
##  (Other) :   0   (Other) :  26   (Other) : 483   (Other) : 271  
##      MBERARBO          MSKA           MSKB1           MSKB2     
##  11 - 23%:1439   0%      :1738   11 - 23%:1783   11 - 23%:1676  
##  24 - 36%:1109   1 - 10% :1569   1 - 10% :1480   24 - 36%:1175  
##  1 - 10% : 980   11 - 23%:1198   0%      :1353   0%      : 990  
##  0%      : 968   24 - 36%: 685   24 - 36%: 775   1 - 10% : 861  
##  37 - 49%: 772   37 - 49%: 261   37 - 49%: 298   37 - 49%: 652  
##  50 - 62%: 331   50 - 62%: 127   50 - 62%:  78   50 - 62%: 357  
##  (Other) : 223   (Other) : 244   (Other) :  55   (Other) : 111  
##        MSKC            MSKD           MHHUUR          MHKOOP    
##  50 - 62%:1168   0%      :2607   0%      : 949   100%    : 949  
##  37 - 49%:1159   1 - 10% :1563   100%    : 760   0%      : 760  
##  24 - 36%:1090   11 - 23%: 852   11 - 23%: 717   76 - 88%: 724  
##  11 - 23%: 870   24 - 36%: 441   24 - 36%: 593   63 - 75%: 604  
##  63 - 75%: 487   37 - 49%: 223   89 - 99%: 532   1 - 10% : 530  
##  0%      : 364   50 - 62%: 100   50 - 62%: 519   50 - 62%: 520  
##  (Other) : 684   (Other) :  36   (Other) :1752   (Other) :1735  
##       MAUT1           MAUT2           MAUT0          MZFONDS    
##  63 - 75%:1663   0%      :1854   11 - 23%:1625   76 - 88%:1511  
##  76 - 88%:1413   11 - 23%:1748   0%      :1450   50 - 62%: 974  
##  50 - 62%:1210   1 - 10% :1468   24 - 36%:1066   63 - 75%: 875  
##  100%    : 505   24 - 36%: 385   1 - 10% : 776   100%    : 852  
##  37 - 49%: 448   37 - 49%: 301   37 - 49%: 587   89 - 99%: 699  
##  89 - 99%: 261   50 - 62%:  56   50 - 62%: 174   37 - 49%: 357  
##  (Other) : 322   (Other) :  10   (Other) : 144   (Other) : 554  
##       MZPART         MINKM30         MINK3045        MINK4575   
##  11 - 23%:1511   0%      :1304   37 - 49%:1356   24 - 36%:1215  
##  37 - 49%: 992   11 - 23%:1094   24 - 36%:1147   11 - 23%:1165  
##  0%      : 852   24 - 36%:1079   50 - 62%: 931   37 - 49%:1034  
##  24 - 36%: 849   1 - 10% : 630   11 - 23%: 919   0%      : 891  
##  1 - 10% : 699   37 - 49%: 599   0%      : 465   1 - 10% : 657  
##  50 - 62%: 364   50 - 62%: 568   63 - 75%: 406   50 - 62%: 498  
##  (Other) : 555   (Other) : 548   (Other) : 598   (Other) : 362  
##      MINK7512        MINK123M        MINKGEM         MKOOPKLA   
##  0%      :3246   0%      :4900   24 - 36%:1932   24 - 36%:1524  
##  1 - 10% :1359   1 - 10% : 763   37 - 49%:1854   37 - 49%: 902  
##  11 - 23%: 736   11 - 23%:  96   50 - 62%: 733   63 - 75%: 901  
##  24 - 36%: 246   24 - 36%:  36   11 - 23%: 651   1 - 10% : 587  
##  37 - 49%: 147   37 - 49%:  24   63 - 75%: 355   50 - 62%: 583  
##  50 - 62%:  71   50 - 62%:   1   76 - 88%: 131   76 - 88%: 474  
##  (Other) :  17   (Other) :   2   (Other) : 166   (Other) : 851  
##       PWAPART            PWABEDR          PWALAND              PPERSAUT   
##  f 0      :3482   f 0        :5740   f 0      :5702   f 0          :2845  
##  f 50-99  :2128   f 50-99    :  30   f 200-499:  60   f 1000-4999  :2319  
##  f 1-49   : 201   f 100-199  :  23   f 100-199:  57   f 500-999    : 613  
##  f 100-199:  11   f 200-499  :  17   f 50-99  :   3   f 5000-9999  :  41  
##  f 200-499:   0   f 1-49     :   7   f 1-49   :   0   f 10000-19999:   3  
##  f 500-999:   0   f 1000-4999:   4   f 500-999:   0   f 200-499    :   1  
##  (Other)  :   0   (Other)    :   1   (Other)  :   0   (Other)      :   0  
##         PBESAUT            PMOTSCO              AAUT           PAANHANG   
##  f 0        :5774   f 0        :5600   f 0        :5813   f 0      :5757  
##  f 1000-4999:  35   f 200-499  : 136   f 1000-4999:   7   f 50-99  :  38  
##  f 500-999  :  10   f 1000-4999:  49   f 200-499  :   1   f 1-49   :  19  
##  f 5000-9999:   3   f 500-999  :  32   f 20000-?  :   1   f 100-199:   6  
##  f 1-49     :   0   f 100-199  :   3   f 1-49     :   0   f 200-499:   1  
##  f 50-99    :   0   f 5000-9999:   2   f 50-99    :   0   f 500-999:   1  
##  (Other)    :   0   (Other)    :   0   (Other)    :   0   (Other)  :   0  
##         PTRACTOR            PWERKT             PBROM     
##  f 0        :5679   f 0        :5801   f 0        :5426  
##  f 100-199  :  79   f 200-499  :   8   f 100-199  : 282  
##  f 500-999  :  28   f 100-199  :   6   f 200-499  :  63  
##  f 200-499  :  27   f 50-99    :   4   f 50-99    :  34  
##  f 1000-4999:   9   f 1000-4999:   3   f 500-999  :  16  
##  f 1-49     :   0   f 1-49     :   0   f 1000-4999:   1  
##  (Other)    :   0   (Other)    :   0   (Other)    :   0  
##          PLEVEN            PPERSONG         PGEZONG            PWAOREG    
##  f 0        :5529   f 0        :5791   f 0      :5784   f 0        :5799  
##  f 200-499  :  94   f 50-99    :  18   f 50-99  :  25   f 1000-4999:  19  
##  f 100-199  :  84   f 100-199  :   4   f 100-199:  13   f 5000-9999:   2  
##  f 1000-4999:  38   f 1-49     :   3   f 1-49   :   0   f 200-499  :   1  
##  f 500-999  :  35   f 200-499  :   3   f 200-499:   0   f 500-999  :   1  
##  f 50-99    :  28   f 1000-4999:   2   f 500-999:   0   f 1-49     :   0  
##  (Other)    :  14   (Other)    :   1   (Other)  :   0   (Other)    :   0  
##          PBRAND          PZEILPL            PPLEZIER          PFIETS    
##  f 0        :2666   f 0      :5819   f 0        :5789   f 0      :5675  
##  f 200-499  :1226   f 1-49   :   2   f 200-499  :  13   f 1-49   : 147  
##  f 100-199  : 920   f 100-199:   1   f 1-49     :   5   f 50-99  :   0  
##  f 50-99    : 535   f 50-99  :   0   f 50-99    :   5   f 100-199:   0  
##  f 1-49     : 161   f 200-499:   0   f 100-199  :   5   f 200-499:   0  
##  f 1000-4999: 155   f 500-999:   0   f 1000-4999:   3   f 500-999:   0  
##  (Other)    : 159   (Other)  :   0   (Other)    :   2   (Other)  :   0  
##       PINBOED          PBYSTAND    AWAPART  AWABEDR  AWALAND  APERSAUT
##  f 0      :5777   f 0      :5740   0:3482   0:5740   0:5702   0:2845  
##  f 1-49   :  18   f 200-499:  44   1:2334   1:  81   1: 120   1:2712  
##  f 50-99  :  16   f 100-199:  22   2:   6   5:   1            2: 246  
##  f 100-199:   6   f 50-99  :  15                              3:  12  
##  f 200-499:   3   f 500-999:   1                              4:   5  
##  f 500-999:   1   f 1-49   :   0                              6:   1  
##  (Other)  :   1   (Other)  :   0                              7:   1  
##  ABESAUT  AMOTSCO  AVRAAUT  AAANHANG ATRACTOR AWERKT   ABROM    ALEVEN  
##  0:5774   0:5600   0:5813   0:5757   0:5679   0:5801   0:5426   0:5529  
##  1:  40   1: 211   1:   6   1:  59   1: 105   1:  12   1: 382   1: 173  
##  2:   4   2:  10   2:   2   2:   4   2:  29   2:   6   2:  14   2: 100  
##  3:   3   8:   1   3:   1   3:   2   3:   3   3:   2            3:  11  
##  4:   1                              4:   6   6:   1            4:   8  
##                                                                 8:   1  
##                                                                         
##  APERSONG AGEZONG  AWAOREG  ABRAND   AZEILPL  APLEZIER AFIETS   AINBOED 
##  0:5791   0:5784   0:5799   0:2666   0:5819   0:5789   0:5675   0:5777  
##  1:  31   1:  38   1:  19   1:3017   1:   3   1:  31   1: 111   1:  44  
##                    2:   4   2: 126            2:   2   2:  34   2:   1  
##                             3:   7                     3:   2           
##                             4:   3                                      
##                             5:   2                                      
##                             7:   1                                      
##  ABYSTAND        CARAVAN    
##  0:5740   noinsurance:5474  
##  1:  81   insurance  : 348  
##  2:   1                     
##                             
##                             
##                             
## 
</code></pre>

<h3>2.2.2 Univariate EDA</h3>

<h3>Multivariate EDA: Explanatory vs Reponse</h3>

<h4>Bar Plots</h4>

<pre><code class="r, fig.width=4, fig.height=3">for(i in 1:85){
  p &lt;- ggplot(dc, aes(x=dc[,i], fill=CARAVAN)) +
    geom_bar()
  print(p)
}
</code></pre>

<h4>Corrgram</h4>

<p><em>Works with continuous variables</em></p>

<pre><code class="#r">corrgram(dc[1:5,])
</code></pre>

<h4>Association Plot</h4>

<p><em>Works with categorical variables</em>
<em>Paused for the time being to increase speed</em></p>

<pre><code class="#r, fig.width=4, fig.height=4">for(i in 1:85){
  t &lt;- table(dc[,c(i, 86)])
  assoc(t, shade=T)
}
</code></pre>

<h3>Decision Tree EDA</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN ~ ., dc)

summary(dt)
</code></pre>

<pre><code>## Call:
## rpart(formula = CARAVAN ~ ., data = dc)
##   n= 5822 
## 
##   CP nsplit rel error xerror xstd
## 1  0      0         1      0    0
## 
## Node number 1: 5822 observations
##   predicted class=noinsurance  expected loss=0.05977  P(node) =1
##     class counts:  5474   348
##    probabilities: 0.940 0.060
</code></pre>

<pre><code class="r">printcp(dt)
</code></pre>

<pre><code>## 
## Classification tree:
## rpart(formula = CARAVAN ~ ., data = dc)
## 
## Variables actually used in tree construction:
## character(0)
## 
## Root node error: 348/5822 = 0.06
## 
## n= 5822 
## 
##   CP nsplit rel error xerror xstd
## 1  0      0         1      0    0
</code></pre>

<pre><code class="r">plotcp(dt)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-5"/> </p>

<pre><code class="r">
plot(dt, uniform = TRUE, main = &quot;Classification Tree for Caravan&quot;)
</code></pre>

<pre><code>## Error: fit is not a tree, just a root
</code></pre>

<pre><code class="r">text(dt, use.n = TRUE, all = TRUE, cex = 0.8)
</code></pre>

<pre><code>## Error: fit is not a tree, just a root
</code></pre>

<p>It looks like the tree cannot differentiate between the two response classes atm. Will need some way to amplify the signal, perhaps through oversampling.</p>

<h2>2.3 Data Selection</h2>

<h3>PCA</h3>

<p><em>Doesn&#39;t work with categorical data</em></p>

<pre><code class="#r">pca &lt;- princomp(data[1:100,1:5])
</code></pre>

<h2>2.4 Modeling Iter. 1</h2>

<p>We begin by running the data through multiple algorithms on their respective default settings. This allows us to gather initial information on the performance of the algorithms as well as the dataset itself.</p>

<h3>Split Test/Train</h3>

<pre><code class="r">set.seed(123)
mask &lt;- sample(5822, 4075)

train &lt;- dc[mask, ]
test &lt;- dc[-mask, ]
</code></pre>

<h3>Define Formula</h3>

<p>We have to define the formula that we&#39;re modeling upon first. We can do this by joining the column names together into a string, and then converting that string into the &ldquo;formula&rdquo; object.</p>

<pre><code class="#r">y &lt;- names(data)[86]
x &lt;- paste(names(data)[categorical_names], collapse=&#39;+&#39;)
f &lt;- as.formula(paste(y, x, sep=&#39;~&#39;))
</code></pre>

<p>We now have the formula, shown below:</p>

<pre><code class="#r">print(f)
</code></pre>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN ~ ., train, family = binomial)
</code></pre>

<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test)
</code></pre>

<pre><code>## Error: factor MAANTHUI has new levels 8
</code></pre>

<p>As seen above, we get a few warnings, one of which informing us that the algorithm didn&#39;t converge. None of these warnings are fatal so we move on with the prediction phase to see what happens. However, we see here that for <strong>some variables the classes used in training are not present in the test set</strong>. Unfortunately this is a fatal error so we will have to move on.</p>

<p><em>We did convert some of variables to numeric but after a certain point this defeats the premise of the original conversion.</em></p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN ~ ., train, method = &quot;class&quot;)
printcp(dt)
</code></pre>

<pre><code>## 
## Classification tree:
## rpart(formula = CARAVAN ~ ., data = train, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## character(0)
## 
## Root node error: 256/4075 = 0.063
## 
## n= 4075 
## 
##   CP nsplit rel error xerror xstd
## 1  0      0         1      0    0
</code></pre>

<p>No nodes were created and no variables were used&hellip;</p>

<h3>Random Forest</h3>

<pre><code class="r">rf &lt;- randomForest(CARAVAN ~ ., train)
</code></pre>

<pre><code>## Error: Can not handle categorical predictors with more than 32 categories.
</code></pre>

<p>Error that it can&#39;t handle &gt;32 categories. Convert and try with continuous data instead.</p>

<pre><code class="r">train.temp &lt;- train
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN ~ ., train.temp)
rf
</code></pre>

<pre><code>## 
## Call:
##  randomForest(formula = CARAVAN ~ ., data = train.temp) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 9
## 
##         OOB estimate of  error rate: 7.41%
## Confusion matrix:
##             noinsurance insurance class.error
## noinsurance        3769        50     0.01309
## insurance           252         4     0.98438
</code></pre>

<p>It runs now, which is good. However, it only correctly identifies 3 &ldquo;insurance&rdquo; observations correctly (in-sample). Let&#39;s try out-of-sample instead and see how the model fares.</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<pre><code>##              rf.pred
##               noinsurance insurance
##   noinsurance        1637        18
##   insurance            88         4
</code></pre>

<p>Unfortunately the model doesn&#39;t do well against the test data either. </p>

<h3>Naive Bayes</h3>

<p><em>Unreliable due to independence requirement. Skipping.</em></p>

<pre><code class="#r">nb &lt;- naiveBayes(CARAVAN~., train)
</code></pre>

<h3>Neural Net</h3>

<pre><code class="r">nn &lt;- nnet(CARAVAN ~ ., train, size = 1)
</code></pre>

<pre><code>## # weights:  657
## initial  value 2423.323436 
## iter  10 value 923.506306
## iter  20 value 820.282895
## iter  30 value 762.422357
## iter  40 value 734.047803
## iter  50 value 722.150546
## iter  60 value 709.833005
## iter  70 value 701.236874
## iter  80 value 691.925175
## iter  90 value 682.522072
## iter 100 value 677.202265
## final  value 677.202265 
## stopped after 100 iterations
</code></pre>

<pre><code class="r">nn.pred &lt;- predict(nn, test, type = &quot;class&quot;)
table(test$CARAVAN, nn.pred)
</code></pre>

<pre><code>##              nn.pred
##               noinsurance
##   noinsurance        1655
##   insurance            92
</code></pre>

<p>The neural has predicted all the test observations to be &#39;noinsurance&#39;. This is probably due to the difference in class counts in the observations.</p>

<h3>Summary</h3>

<p>None of the techniques used in this iteration provided satisfactory results. To sum up the findings:</p>

<ul>
<li>Logistic Regression

<ul>
<li>Returned warnings, plus the nature of the model requires the same dummy variables in both training and testing. This was not the case.</li>
</ul></li>
<li>Decision Tree:

<ul>
<li>Nodes weren&#39;t even created. Probably due to the large skew of the CARAVAN class.</li>
</ul></li>
<li>Random Forest

<ul>
<li>Horrible accuracy.</li>
</ul></li>
<li>Neural Net

<ul>
<li>Did not assign a single observation to &#39;insurance&#39;. </li>
</ul></li>
</ul>

<h2>2.5 Modeling Iter. 2</h2>

<p>We discovered in iteration 1 that the ratio between the classes is just too large. The algorithms will simply settle on 0. We will need to oversample this dataset to even out the obervations between both classes. This way the algorithms will ignore the difference in sample size between the 2.</p>

<h3>Oversample</h3>

<pre><code class="r">train.over &lt;- train
table(train.over$CARAVAN)
</code></pre>

<pre><code>## 
## noinsurance   insurance 
##        3819         256
</code></pre>

<p>Before oversampling, there are 256 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. To make them roughly the same we will repeat the &ldquo;insurance&rdquo; observations 14 times.</p>

<pre><code class="r">temp &lt;- train.over[grep(&quot;^insurance&quot;, train.over$CARAVAN), ]
for (i in 1:14) {
    train.over &lt;- rbind(train.over, temp)
}
table(train.over$CARAVAN)
</code></pre>

<pre><code>## 
## noinsurance   insurance 
##        3819        3840
</code></pre>

<p>There are now 3840 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. Let&#39;s try some of the same models again.</p>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN ~ ., train.over, family = binomial)
</code></pre>

<pre><code>## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>Still the same errors as before: &ldquo;algorithm did not converge&rdquo;</p>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test, type = &quot;response&quot;)
</code></pre>

<pre><code>## Error: factor MAANTHUI has new levels 8
</code></pre>

<pre><code class="r">table(lrm.pred, test$CARAVAN)
</code></pre>

<pre><code>## Error: object &#39;lrm.pred&#39; not found
</code></pre>

<p>Nope, not even working.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN ~ ., train.over, method = &quot;class&quot;)
printcp(dt)
</code></pre>

<pre><code>## 
## Classification tree:
## rpart(formula = CARAVAN ~ ., data = train.over, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] MHHUUR   PBRAND   PPERSAUT STYPE   
## 
## Root node error: 3819/7659 = 0.5
## 
## n= 7659 
## 
##      CP nsplit rel error xerror  xstd
## 1 0.371      0      1.00   1.03 0.011
## 2 0.042      1      0.63   0.63 0.011
## 3 0.011      2      0.59   0.58 0.010
## 4 0.010      4      0.56   0.56 0.010
</code></pre>

<p>Much better this time. 4 variables were used in tree construction this time.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type = &quot;class&quot;)
table(dt.pred, test$CARAVAN)
</code></pre>

<pre><code>##              
## dt.pred       noinsurance insurance
##   noinsurance        1130        28
##   insurance           525        64
</code></pre>

<p>Unfortunately the results still aren&#39;t the best. The false negative rate is huge 525/1655 and the false positive rate is 28/92. </p>

<h3>Random Forest</h3>

<pre><code class="r">train.temp &lt;- train.over
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN ~ ., train.temp)
</code></pre>

<p>These in-sample results are great! This time the forest was able to correctly identify all of the &ldquo;insurance&rdquo; observations (though they are oversampled).</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<pre><code>##              rf.pred
##               noinsurance insurance
##   noinsurance        1573        82
##   insurance            83         9
</code></pre>

<p>When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically&hellip;</p>

<h3>Neural Net</h3>

<pre><code class="r">nn &lt;- nnet(CARAVAN ~ ., train.over, size = 1)
</code></pre>

<pre><code>## # weights:  657
## initial  value 5316.038904 
## iter  10 value 4147.771607
## iter  20 value 3739.432462
## iter  30 value 3495.464751
## iter  40 value 3329.975539
## iter  50 value 3254.903831
## iter  60 value 3211.337072
## iter  70 value 3176.527704
## iter  80 value 3148.131185
## iter  90 value 3109.653196
## iter 100 value 3107.632459
## final  value 3107.632459 
## stopped after 100 iterations
</code></pre>

<pre><code class="r">nn.pred &lt;- predict(nn, test, type = &quot;class&quot;)
table(test$CARAVAN, nn.pred)
</code></pre>

<pre><code>##              nn.pred
##               insurance noinsurance
##   noinsurance       166        1489
##   insurance          20          72
</code></pre>

<h2>Modeling Iter. 3</h2>

<p>Now that we have the algorithms working due to oversampling, we&#39;re running into actual modeling problems. The primary issue we&#39;re observing is the large false positive and false negative rates.</p>

<h3>Feature selection via DT EDA</h3>

<p>MHHUUR   PBRAND   PPERSAUT STYPE</p>

<pre><code class="r">dt_cols &lt;- c(&quot;CARAVAN&quot;, &quot;MHHUUR&quot;, &quot;PPERSAUT&quot;, &quot;STYPE&quot;)
train.dt &lt;- train.over[, dt_cols]
test.dt &lt;- test[, dt_cols]
</code></pre>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN ~ ., train.dt, family = binomial)
</code></pre>

<p>No more &ldquo;algorithm did not converge&rdquo; warning.</p>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test.dt)
lrm.pred &lt;- 1 * (lrm.pred &gt;= 0.5)
table(lrm.pred, test$CARAVAN)
</code></pre>

<pre><code>##         
## lrm.pred noinsurance insurance
##        0        1277        43
##        1         378        49
</code></pre>

<p>Working, but not that great.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN ~ ., train.dt, method = &quot;class&quot;)
printcp(dt)
</code></pre>

<pre><code>## 
## Classification tree:
## rpart(formula = CARAVAN ~ ., data = train.dt, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] MHHUUR   PPERSAUT STYPE   
## 
## Root node error: 3819/7659 = 0.5
## 
## n= 7659 
## 
##      CP nsplit rel error xerror  xstd
## 1 0.371      0      1.00   1.03 0.011
## 2 0.042      1      0.63   0.63 0.011
## 3 0.012      2      0.59   0.61 0.011
## 4 0.010      5      0.55   0.59 0.010
</code></pre>

<p>Not surprisingly, the DT still works and uses all four of the variables we selected out.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type = &quot;class&quot;)
table(dt.pred, test$CARAVAN)
</code></pre>

<pre><code>##              
## dt.pred       noinsurance insurance
##   noinsurance        1039        26
##   insurance           616        66
</code></pre>

<p>And also not too surprisingly, the results are a bit worse give we removed a ton of information. The false negative rate is now a huge 649/1655 and the false positive rate is 21/92. </p>

<h3>Random Forest</h3>

<pre><code class="r">train.temp &lt;- train.dt
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN ~ ., train.temp)
</code></pre>

<pre><code class="r">test.temp &lt;- test.dt
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
</code></pre>

<pre><code>## Error: New factor levels not present in the training data
</code></pre>

<pre><code class="r">table(test$CARAVAN, rf.pred)
</code></pre>

<pre><code>##              rf.pred
##               noinsurance insurance
##   noinsurance        1573        82
##   insurance            83         9
</code></pre>

<h3>Summary</h3>

<p>It looks like doing a feature selection through decision tree EDA just isn&#39;t that great. Let&#39;s explore some other FS techniques.</p>

<h2>Modeling Iter. 4</h2>

<p>While oversampling did balance our dataset out enough to enable actual modeling, undersampling may be better due to the underlying methodology. Instead of producing fake date we will randomly sample the &#39;noinsurance&#39; class to even out the balance.</p>

<h3>Undersample</h3>

<pre><code class="r">train.over &lt;- train
table(train.over$CARAVAN)
</code></pre>

<pre><code>## 
## noinsurance   insurance 
##        3819         256
</code></pre>

<p>Before oversampling, there are 256 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. To make them roughly the same we will repeat the &ldquo;insurance&rdquo; observations 14 times.</p>

<pre><code class="r">temp &lt;- train.over[grep(&quot;^insurance&quot;, train.over$CARAVAN), ]
for (i in 1:14) {
    train.over &lt;- rbind(train.over, temp)
}
table(train.over$CARAVAN)
</code></pre>

<pre><code>## 
## noinsurance   insurance 
##        3819        3840
</code></pre>

<p>There are now 3840 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. Let&#39;s try some of the same models again.</p>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN ~ ., train.over, family = binomial)
</code></pre>

<pre><code>## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>Still the same errors as before: &ldquo;algorithm did not converge&rdquo;</p>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test, type = &quot;response&quot;)
</code></pre>

<pre><code>## Error: factor MAANTHUI has new levels 8
</code></pre>

<pre><code class="r">table(lrm.pred, test$CARAVAN)
</code></pre>

<pre><code>##         
## lrm.pred noinsurance insurance
##        0        1277        43
##        1         378        49
</code></pre>

<p>Nope, not even working.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN ~ ., train.over, method = &quot;class&quot;)
printcp(dt)
</code></pre>

<pre><code>## 
## Classification tree:
## rpart(formula = CARAVAN ~ ., data = train.over, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] MHHUUR   PBRAND   PPERSAUT STYPE   
## 
## Root node error: 3819/7659 = 0.5
## 
## n= 7659 
## 
##      CP nsplit rel error xerror  xstd
## 1 0.371      0      1.00   1.02 0.011
## 2 0.042      1      0.63   0.63 0.011
## 3 0.011      2      0.59   0.59 0.010
## 4 0.010      4      0.56   0.57 0.010
</code></pre>

<p>Much better this time. 4 variables were used in tree construction this time.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type = &quot;class&quot;)
table(dt.pred, test$CARAVAN)
</code></pre>

<pre><code>##              
## dt.pred       noinsurance insurance
##   noinsurance        1130        28
##   insurance           525        64
</code></pre>

<p>Unfortunately the results still aren&#39;t the best. The false negative rate is huge 525/1655 and the false positive rate is 28/92. </p>

<h3>Random Forest</h3>

<pre><code class="r">train.temp &lt;- train.over
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN ~ ., train.temp)
</code></pre>

<p>These in-sample results are great! This time the forest was able to correctly identify all of the &ldquo;insurance&rdquo; observations (though they are oversampled).</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<pre><code>##              rf.pred
##               noinsurance insurance
##   noinsurance        1572        83
##   insurance            84         8
</code></pre>

<p>When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically&hellip;</p>

<h3>Neural Net</h3>

<pre><code class="r">nn &lt;- nnet(CARAVAN ~ ., train.over, size = 1)
</code></pre>

<pre><code>## # weights:  657
## initial  value 5427.194377 
## iter  10 value 4950.103929
## iter  20 value 4758.362414
## iter  30 value 4611.611315
## iter  40 value 4418.542374
## iter  50 value 4146.689057
## iter  60 value 3800.865643
## iter  70 value 3528.552587
## iter  80 value 3214.190397
## iter  90 value 3058.417840
## iter 100 value 2890.844824
## final  value 2890.844824 
## stopped after 100 iterations
</code></pre>

<pre><code class="r">nn.pred &lt;- predict(nn, test, type = &quot;class&quot;)
table(test$CARAVAN, nn.pred)
</code></pre>

<pre><code>##              nn.pred
##               insurance noinsurance
##   noinsurance       343        1312
##   insurance          34          58
</code></pre>

<h2>Modeling Iter. 5</h2>

<p>Feature selection</p>

</body>

</html>

