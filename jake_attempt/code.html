<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>1. Introduction</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<p><strong>Setup</strong></p>

<pre><code class="r">library(kernlab)
#library(car)
library(ggplot2)
library(corrgram)
library(vcd)
library(rpart)
library(randomForest)
library(e1071)
library(nnet)
library(MASS)
library(bootstrap)
library(DAAG)
</code></pre>

<h1>1. Introduction</h1>

<h1>2. Process</h1>

<h2>2.1 Data Preparation</h2>

<h3>2.1.1 Import Dataset</h3>

<pre><code class="r">data(ticdata)
#raw &lt;- read.csv(&#39;../ticdata2000.csv&#39;)
data &lt;- ticdata[1:5822,]
</code></pre>

<h3>2.1.2 Modify Datatypes</h3>

<pre><code class="r">#categorical_names &lt;- grep(&#39;^m|^caravan&#39;, names(data)) # not needed for kernlab
#data[,categorical_names] &lt;- lapply(data[,categorical_names], as.factor)
dc &lt;- as.data.frame(lapply(data, as.factor)) # transform all to categorical

str(dc)
head(dc)
</code></pre>

<h2>2.2 Data Exploration</h2>

<h3>2.2.1 Summary EDA</h3>

<pre><code class="r">summary(dc)
</code></pre>

<h3>2.2.2 Univariate EDA</h3>

<h3>Multivariate EDA: Explanatory vs Reponse</h3>

<h4>Bar Plots</h4>

<pre><code class="r, fig.width=4, fig.height=3">for(i in 1:85){
  p &lt;- ggplot(dc, aes(x=dc[,i], fill=CARAVAN)) +
    geom_bar() +
    labs(x=names(dc)[i])
  print(p)
}
</code></pre>

<h4>Corrgram</h4>

<p><em>Works with continuous variables</em></p>

<pre><code class="#r">corrgram(dc[1:5,])
</code></pre>

<h4>Association Plot</h4>

<p><em>Works with categorical variables</em>
<em>Paused for the time being to increase speed</em></p>

<pre><code class="r, fig.width=4, fig.height=4">for(i in 1:85){
  t &lt;- table(dc[,c(i, 86)])
  assoc(t, shade=T)
}
</code></pre>

<h3>Decision Tree EDA</h3>

<pre><code class="r, fig.width=10, fig.height=10">dt &lt;- rpart(CARAVAN~., dc)

summary(dt)
printcp(dt)
plotcp(dt)

plot(dt, uniform=TRUE, main=&quot;Classification Tree for Caravan&quot;)
text(dt, use.n=TRUE, all=TRUE, cex=0.8)
</code></pre>

<p>It looks like the tree cannot differentiate between the two response classes atm. Will need some way to amplify the signal, perhaps through oversampling.</p>

<h2>2.3 Data Selection</h2>

<h3>PCA</h3>

<p><em>Doesn&#39;t work with categorical data</em></p>

<pre><code class="#r">pca &lt;- princomp(data[1:100,1:5])
</code></pre>

<h2>2.4 Modeling Iter. 1</h2>

<p>We begin by running the data through multiple algorithms on their respective default settings. This allows us to gather initial information on the performance of the algorithms as well as the dataset itself.</p>

<h3>Split Test/Train</h3>

<pre><code class="r">set.seed(123)
mask &lt;- sample(5822,4075)

train &lt;- dc[mask,]
test &lt;- dc[-mask,]
</code></pre>

<h3>Define Formula</h3>

<p>We have to define the formula that we&#39;re modeling upon first. We can do this by joining the column names together into a string, and then converting that string into the &ldquo;formula&rdquo; object.</p>

<pre><code class="#r">y &lt;- names(data)[86]
x &lt;- paste(names(data)[categorical_names], collapse=&#39;+&#39;)
f &lt;- as.formula(paste(y, x, sep=&#39;~&#39;))
</code></pre>

<p>We now have the formula, shown below:</p>

<pre><code class="#r">print(f)
</code></pre>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN~., train, family=binomial)
lrm.pred &lt;- predict(lrm, test)
</code></pre>

<p>As seen above, we get a few warnings, one of which informing us that the algorithm didn&#39;t converge. None of these warnings are fatal so we move on with the prediction phase to see what happens. However, we see here that for <strong>some variables the classes used in training are not present in the test set</strong>. Unfortunately this is a fatal error so we will have to move on.</p>

<p><em>We did convert some of variables to numeric but after a certain point this defeats the premise of the original conversion.</em></p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN~., train, method=&#39;class&#39;)
printcp(dt)
</code></pre>

<p>No nodes were created and no variables were used&hellip;</p>

<h3>Random Forest</h3>

<pre><code class="r">rf &lt;- randomForest(CARAVAN~., train)
</code></pre>

<p>Error that it can&#39;t handle &gt;32 categories. Convert and try with continuous data instead.</p>

<pre><code class="r">train.temp &lt;- train
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN~., train.temp)
rf
</code></pre>

<p>It runs now, which is good. However, it only correctly identifies 3 &ldquo;insurance&rdquo; observations correctly (in-sample). Let&#39;s try out-of-sample instead and see how the model fares.</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<p>Unfortunately the model doesn&#39;t do well against the test data either. </p>

<h3>Naive Bayes</h3>

<p><em>Unreliable due to independence requirement. Skipping.</em></p>

<pre><code class="#r">nb &lt;- naiveBayes(CARAVAN~., train)
</code></pre>

<h3>Neural Net</h3>

<pre><code class="r">nn &lt;- nnet(CARAVAN~., train, size=1)
nn.pred &lt;- predict(nn, test, type=&#39;class&#39;)
table(test$CARAVAN, nn.pred)
</code></pre>

<p>The neural has predicted all the test observations to be &#39;noinsurance&#39;. This is probably due to the difference in class counts in the observations.</p>

<h3>Summary</h3>

<p>None of the techniques used in this iteration provided satisfactory results. To sum up the findings:</p>

<ul>
<li>Logistic Regression

<ul>
<li>Returned warnings, plus the nature of the model requires the same dummy variables in both training and testing. This was not the case.</li>
</ul></li>
<li>Decision Tree:

<ul>
<li>Nodes weren&#39;t even created. Probably due to the large skew of the CARAVAN class.</li>
</ul></li>
<li>Random Forest

<ul>
<li>Horrible accuracy.</li>
</ul></li>
<li>Neural Net

<ul>
<li>Did not assign a single observation to &#39;insurance&#39;. </li>
</ul></li>
</ul>

<h2>2.5 Modeling Iter. 2</h2>

<p>We discovered in iteration 1 that the ratio between the classes is just too large. The algorithms will simply settle on 0. We will need to oversample this dataset to even out the obervations between both classes. This way the algorithms will ignore the difference in sample size between the 2.</p>

<h3>Oversample</h3>

<pre><code class="r">train.over &lt;- train
table(train.over$CARAVAN)
</code></pre>

<p>Before oversampling, there are 256 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. To make them roughly the same we will repeat the &ldquo;insurance&rdquo; observations 14 times.</p>

<pre><code class="r">temp &lt;- train.over[grep(&#39;^insurance&#39;, train.over$CARAVAN),]
for(i in 1:14){
  train.over &lt;- rbind(train.over, temp)
}
table(train.over$CARAVAN)
</code></pre>

<p>There are now 3840 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. Let&#39;s try some of the same models again.</p>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN~., train.over, family=binomial)
</code></pre>

<p>Still the same errors as before: &ldquo;algorithm did not converge&rdquo;</p>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test, type=&#39;response&#39;)
table(lrm.pred, test$CARAVAN)
</code></pre>

<p>Nope, not even working.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN~., train.over, method=&#39;class&#39;)
printcp(dt)
</code></pre>

<p>Much better this time. 4 variables were used in tree construction this time.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type=&#39;class&#39;)
table(dt.pred, test$CARAVAN)
</code></pre>

<p>Unfortunately the results still aren&#39;t the best. The false negative rate is huge 525/1655 and the false positive rate is 28/92. </p>

<h3>Random Forest</h3>

<pre><code class="r">train.temp &lt;- train.over
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN~., train.temp)
rf
</code></pre>

<p>These in-sample results are great! This time the forest was able to correctly identify all of the &ldquo;insurance&rdquo; observations (though they are oversampled).</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<p>When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically&hellip;</p>

<h3>Neural Net</h3>

<pre><code class="r">system.time(nn &lt;- nnet(CARAVAN~., train.over, size=1))
nn.pred &lt;- predict(nn, test, type=&#39;class&#39;)
table(test$CARAVAN, nn.pred)
</code></pre>

<h2>Modeling Iter. 3</h2>

<p>Now that we have the algorithms working due to oversampling, we&#39;re running into actual modeling problems. The primary issue we&#39;re observing is the large false positive and false negative rates.</p>

<h3>Feature selection via DT EDA</h3>

<p>MHHUUR   PBRAND   PPERSAUT STYPE</p>

<pre><code class="r">dt_cols &lt;- c(&#39;CARAVAN&#39;, &#39;MHHUUR&#39;, &#39;PPERSAUT&#39;, &#39;STYPE&#39;)
train.dt &lt;- train.over[, dt_cols]
test.dt &lt;- test[, dt_cols]
</code></pre>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN~., train.dt, family=binomial)
</code></pre>

<p>No more &ldquo;algorithm did not converge&rdquo; warning.</p>

<pre><code class="r">lrm.pred &lt;- predict(lrm, test.dt)
lrm.pred &lt;- 1*(lrm.pred &gt;= 0.5)
table(lrm.pred, test$CARAVAN)
</code></pre>

<p>Working, but not that great.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN~., train.dt, method=&#39;class&#39;)
printcp(dt)
</code></pre>

<p>Not surprisingly, the DT still works and uses all four of the variables we selected out.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type=&#39;class&#39;)
table(dt.pred, test$CARAVAN)
</code></pre>

<p>And also not too surprisingly, the results are a bit worse give we removed a ton of information. The false negative rate is now a huge 649/1655 and the false positive rate is 21/92. </p>

<h3>Random Forest</h3>

<pre><code class="r">train.temp &lt;- train.dt
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN~., train.temp)
rf
</code></pre>

<pre><code class="r">test.temp &lt;- test.dt
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<h3>Summary</h3>

<p>It looks like doing a feature selection through decision tree EDA just isn&#39;t that great. Let&#39;s explore some other FS techniques.</p>

<h2>Modeling Iter. 4</h2>

<p>First, let&#39;s explore undersampling.
While oversampling did balance our dataset out enough to enable actual modeling, undersampling may be better due to the underlying methodology. Instead of producing fake date we will randomly sample the &#39;noinsurance&#39; class to even out the balance.</p>

<h3>Undersample</h3>

<pre><code class="r">train.under &lt;- train
table(train.under$CARAVAN)
</code></pre>

<p>Like before, we start off with there are 256 &ldquo;insurance&rdquo; and 3819 &ldquo;noinsurance&rdquo;. To make this balanced we will randomly sample 256 &ldquo;noinsurance&rdquo; observations.</p>

<pre><code class="r">mask &lt;- sample(3819, 256)
temp &lt;- subset(train.under, CARAVAN %in% &#39;noinsurance&#39;)
temp &lt;- temp[mask,]
train.under &lt;- rbind(temp, subset(train.under, CARAVAN %in% &#39;insurance&#39;))

table(train.under$CARAVAN)
</code></pre>

<p>There are now 256 &ldquo;insurance&rdquo; and 256 &ldquo;noinsurance&rdquo;. Let&#39;s try some of the same models again.</p>

<p>IGNORE
The truncating of the dataset has <strong>cut down observations of classes within some variables#</strong>. This means that we will have to redo the factors inside the dataset.
/IGNORE</p>

<h3>Logistic Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN~., train.under, family=binomial)
</code></pre>

<p>New error this time: contrasts can be applied only to factors with 2 or more levels
This means that we&#39;ve cut down too much data for the logistic regression to actually process&hellip; Let&#39;s try something else for now.</p>

<h3>Decision Tree</h3>

<pre><code class="r">dt &lt;- rpart(CARAVAN~., train.under, method=&#39;class&#39;)
printcp(dt)
</code></pre>

<p>The model actually produced results this time. Let&#39;s test it.</p>

<pre><code class="r">dt.pred &lt;- predict(dt, test, type=&#39;class&#39;)
table(dt.pred, test$CARAVAN)
</code></pre>

<p>These results are comparable to the oversampling trial before.. The false negative rate is huge 641/1655 and the false positive rate is 28/92. </p>

<h3>Random Forest</h3>

<pre><code class="">train.temp &lt;- train.under
train.temp$STYPE &lt;- as.numeric(train.temp$STYPE)
rf &lt;- randomForest(CARAVAN~., train.temp)
rf
</code></pre>

<p>Not the best</p>

<pre><code class="r">test.temp &lt;- test
test.temp$STYPE &lt;- as.numeric(test.temp$STYPE)
rf.pred &lt;- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
</code></pre>

<p>When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically&hellip;</p>

<h3>Neural Net</h3>

<pre><code class="r">system.time(nn &lt;- nnet(CARAVAN~., train.under, size=1))
nn.pred &lt;- predict(nn, test, type=&#39;class&#39;)
table(test$CARAVAN, nn.pred)
</code></pre>

<p>Runs drastically faster, which isn&#39;t surprising. The algorithm also <strong>returns only &ldquo;insurance&rdquo; classes</strong> for entirety of the test set. It seems the undersampling did not work for NN.</p>

<h3>Summary</h3>

<p>Recommend tossing train.under dataset. With just 512 observations there&#39;s just too much information lost.</p>

<h2>Modeling Iter. 5</h2>

<p>Feature selection
Continue with oversampled set</p>

<h3>Stepwise Regression</h3>

<pre><code class="r">lrm &lt;- glm(CARAVAN~., train, family=binomial)
step &lt;- stepAIC(lrm, directions=&#39;both&#39;)
</code></pre>

<p>This takes forever&hellip;</p>

<h3>Manual</h3>

<pre><code class="r">lrm1 &lt;- glm(CARAVAN~STYPE, train.over, family=binomial)
lrm2 &lt;- glm(CARAVAN~MAANTHUI, train.over, family=binomial)
anova(lrm1, lrm2)
</code></pre>

<h2>Other Tasks</h2>

<p>Cross Validation: may help with the skew of the target classe
Indicator Binning: to solve the Curse of Dimensionality</p>

</body>

</html>

