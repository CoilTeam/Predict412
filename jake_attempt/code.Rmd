**Setup**
```{r}
library(kernlab)
#library(car)
library(ggplot2)
library(corrgram)
library(vcd)
library(rpart)
library(randomForest)
library(e1071)
library(nnet)
```

1. Introduction
============


2. Process
=======
2.1 Data Preparation
----------------
### 2.1.1 Import Dataset
```{r}
data(ticdata)
#raw <- read.csv('../ticdata2000.csv')
data <- ticdata[1:5822,]
```
### 2.1.2 Modify Datatypes
```{r}
#categorical_names <- grep('^m|^caravan', names(data)) # not needed for kernlab
#data[,categorical_names] <- lapply(data[,categorical_names], as.factor)
dc <- as.data.frame(lapply(data, as.factor)) # transform all to categorical

str(dc)
head(dc)
```


2.2 Data Exploration
----------------
### 2.2.1 Summary EDA
```{r}
summary(dc)
```

### 2.2.2 Univariate EDA

### Multivariate EDA: Explanatory vs Reponse
#### Bar Plots
```{.r, fig.width=4, fig.height=3}
for(i in 1:85){
  p <- ggplot(dc, aes(x=dc[,i], fill=CARAVAN)) +
    geom_bar()
  print(p)
}
```

#### Corrgram
_Works with continuous variables_
```{#r}
corrgram(dc[1:5,])
```
#### Association Plot
_Works with categorical variables_
_Paused for the time being to increase speed_
```{#r, fig.width=4, fig.height=4}
for(i in 1:85){
  t <- table(dc[,c(i, 86)])
  assoc(t, shade=T)
}
```

### Decision Tree EDA
```{r, fig.width=10, fig.height=10}
dt <- rpart(CARAVAN~., dc)

summary(dt)
printcp(dt)
plotcp(dt)

plot(dt, uniform=TRUE, main="Classification Tree for Caravan")
text(dt, use.n=TRUE, all=TRUE, cex=0.8)
```
It looks like the tree cannot differentiate between the two response classes atm. Will need some way to amplify the signal, perhaps through oversampling.

2.3 Data Selection
------------------
### PCA
_Doesn't work with categorical data_
```{#r}
pca <- princomp(data[1:100,1:5])
```


2.4 Modeling Iter. 1
--------------------
We begin by running the data through multiple algorithms on their respective default settings. This allows us to gather initial information on the performance of the algorithms as well as the dataset itself.
### Split Test/Train
```{r}
set.seed(123)
mask <- sample(5822,4075)

train <- dc[mask,]
test <- dc[-mask,]
```

### Define Formula
We have to define the formula that we're modeling upon first. We can do this by joining the column names together into a string, and then converting that string into the "formula" object.
```{#r}
y <- names(data)[86]
x <- paste(names(data)[categorical_names], collapse='+')
f <- as.formula(paste(y, x, sep='~'))
```
We now have the formula, shown below:
```{#r}
print(f)
```

### Logistic Regression
```{r}
lrm <- glm(CARAVAN~., train, family=binomial)
lrm.pred <- predict(lrm, test)
```
As seen above, we get a few warnings, one of which informing us that the algorithm didn't converge. None of these warnings are fatal so we move on with the prediction phase to see what happens. However, we see here that for **some variables the classes used in training are not present in the test set**. Unfortunately this is a fatal error so we will have to move on.

_We did convert some of variables to numeric but after a certain point this defeats the premise of the original conversion._

### Decision Tree
```{r}
dt <- rpart(CARAVAN~., train, method='class')
printcp(dt)
```
No nodes were created and no variables were used...

### Random Forest
```{r}
rf <- randomForest(CARAVAN~., train)
```
Error that it can't handle >32 categories. Convert and try with continuous data instead.
```{r}
train.temp <- train
train.temp$STYPE <- as.numeric(train.temp$STYPE)
rf <- randomForest(CARAVAN~., train.temp)
rf
```
It runs now, which is good. However, it only correctly identifies 3 "insurance" observations correctly (in-sample). Let's try out-of-sample instead and see how the model fares.
```{r}
test.temp <- test
test.temp$STYPE <- as.numeric(test.temp$STYPE)
rf.pred <- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
```
Unfortunately the model doesn't do well against the test data either. 

### Naive Bayes
_Unreliable due to independence requirement. Skipping._
```{#r}
nb <- naiveBayes(CARAVAN~., train)
```

### Neural Net
```{r}
nn <- nnet(CARAVAN~., train, size=1)
nn.pred <- predict(nn, test, type='class')
table(test$CARAVAN, nn.pred)
```
The neural has predicted all the test observations to be 'noinsurance'. This is probably due to the difference in class counts in the observations.

### Summary
None of the techniques used in this iteration provided satisfactory results. To sum up the findings:
- Logistic Regression
  - Returned warnings, plus the nature of the model requires the same dummy variables in both training and testing. This was not the case.
- Decision Tree:
  - Nodes weren't even created. Probably due to the large skew of the CARAVAN class.
- Random Forest
  - Horrible accuracy.
- Neural Net
  - Did not assign a single observation to 'insurance'. 


2.5 Modeling Iter. 2
--------------------
We discovered in iteration 1 that the ratio between the classes is just too large. The algorithms will simply settle on 0. We will need to oversample this dataset to even out the obervations between both classes. This way the algorithms will ignore the difference in sample size between the 2.
### Oversample
```{r}
train.over <- train
table(train.over$CARAVAN)
```
Before oversampling, there are 256 "insurance" and 3819 "noinsurance". To make them roughly the same we will repeat the "insurance" observations 14 times.
```{r}
temp <- train.over[grep('^insurance', train.over$CARAVAN),]
for(i in 1:14){
  train.over <- rbind(train.over, temp)
}
table(train.over$CARAVAN)
```
There are now 3840 "insurance" and 3819 "noinsurance". Let's try some of the same models again.

### Logistic Regression
```{r}
lrm <- glm(CARAVAN~., train.over, family=binomial)
```
Still the same errors as before: "algorithm did not converge"
```{r}
lrm.pred <- predict(lrm, test, type='response')
table(lrm.pred, test$CARAVAN)
```
Nope, not even working.

### Decision Tree
```{r}
dt <- rpart(CARAVAN~., train.over, method='class')
printcp(dt)
```
Much better this time. 4 variables were used in tree construction this time.
```{r}
dt.pred <- predict(dt, test, type='class')
table(dt.pred, test$CARAVAN)
```
Unfortunately the results still aren't the best. The false negative rate is huge 525/1655 and the false positive rate is 28/92. 

### Random Forest
```{r}
train.temp <- train.over
train.temp$STYPE <- as.numeric(train.temp$STYPE)
rf <- randomForest(CARAVAN~., train.temp)
rf
```
These in-sample results are great! This time the forest was able to correctly identify all of the "insurance" observations (though they are oversampled).
```{r}
test.temp <- test
test.temp$STYPE <- as.numeric(test.temp$STYPE)
rf.pred <- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
```
When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically...

### Neural Net
```{r}
nn <- nnet(CARAVAN~., train.over, size=1)
```
```{r}
nn.pred <- predict(nn, test, type='class')
table(test$CARAVAN, nn.pred)
```


Modeling Iter. 3
----------------
Now that we have the algorithms working due to oversampling, we're running into actual modeling problems. The primary issue we're observing is the large false positive and false negative rates.

### Feature selection via DT EDA
MHHUUR   PBRAND   PPERSAUT STYPE
```{r}
dt_cols <- c('CARAVAN', 'MHHUUR', 'PPERSAUT', 'STYPE')
train.dt <- train.over[, dt_cols]
test.dt <- test[, dt_cols]
```

### Logistic Regression
```{r}
lrm <- glm(CARAVAN~., train.dt, family=binomial)
```
No more "algorithm did not converge" warning.
```{r}
lrm.pred <- predict(lrm, test.dt)
lrm.pred <- 1*(lrm.pred >= 0.5)
table(lrm.pred, test$CARAVAN)
```
Working, but not that great.

### Decision Tree
```{r}
dt <- rpart(CARAVAN~., train.dt, method='class')
printcp(dt)
```
Not surprisingly, the DT still works and uses all four of the variables we selected out.
```{r}
dt.pred <- predict(dt, test, type='class')
table(dt.pred, test$CARAVAN)
```
And also not too surprisingly, the results are a bit worse give we removed a ton of information. The false negative rate is now a huge 649/1655 and the false positive rate is 21/92. 

### Random Forest
```{r}
train.temp <- train.dt
train.temp$STYPE <- as.numeric(train.temp$STYPE)
rf <- randomForest(CARAVAN~., train.temp)
rf
```

```{r}
test.temp <- test.dt
test.temp$STYPE <- as.numeric(test.temp$STYPE)
rf.pred <- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
```

### Summary
It looks like doing a feature selection through decision tree EDA just isn't that great. Let's explore some other FS techniques.


Modeling Iter. 4
----------------
While oversampling did balance our dataset out enough to enable actual modeling, undersampling may be better due to the underlying methodology. Instead of producing fake date we will randomly sample the 'noinsurance' class to even out the balance.
### Undersample
```{r}
train.under <- train
table(train.under$CARAVAN)
```
Like before, we start off with there are 256 "insurance" and 3819 "noinsurance". To make this balanced we will randomly sample 256 "noinsurance" observations.
```{r}
mask <- sample(3819, 256)
temp <- subset(train.under, CARAVAN %in% 'noinsurance')
temp <- temp[mask,]
train.under <- rbind(temp, subset(train.under, CARAVAN %in% 'insurance'))

table(train.under$CARAVAN)
```
There are now 256 "insurance" and 256 "noinsurance". Let's try some of the same models again.

IGNORE
The truncating of the dataset has **cut down observations of classes within some variables#**. This means that we will have to redo the factors inside the dataset.
/IGNORE


### Logistic Regression
```{r}
lrm <- glm(CARAVAN~., train.under, family=binomial)
```
New error this time: contrasts can be applied only to factors with 2 or more levels
This means that we've cut down too much data for the logistic regression to actually process... Let's try something else for now.

### Decision Tree
```{r}
dt <- rpart(CARAVAN~., train.under, method='class')
printcp(dt)
```
The model actually produced results this time. Let's test it.
```{r}
dt.pred <- predict(dt, test, type='class')
table(dt.pred, test$CARAVAN)
```
These results are comparable to the oversampling trial before.. The false negative rate is huge 641/1655 and the false positive rate is 28/92. 

### Random Forest
```{r}
train.temp <- train.under
train.temp$STYPE <- as.numeric(train.temp$STYPE)
rf <- randomForest(CARAVAN~., train.temp)
rf
```
Not the best
```{r}
test.temp <- test
test.temp$STYPE <- as.numeric(test.temp$STYPE)
rf.pred <- predict(rf, test.temp)
table(test$CARAVAN, rf.pred)
```
When looking at the test set, unfortunately, the out-of-samply accuracy decreases drastically...

### Neural Net
```{r}
nn <- nnet(CARAVAN~., train.over, size=1)
```
```{r}
nn.pred <- predict(nn, test, type='class')
table(test$CARAVAN, nn.pred)
```


Modeling Iter. 5
----------------
Feature selection
