##################################################################################
### 1)  Seed only for balance; Run the original data splits and preparation on the transformed data set
##################################################################################
rm(list=ls())
library(plyr)
library(xlsx)
library(sampling)
library(pls) 
library(MASS)
library(kernlab)
library(ROCR)
library(caret)


#Set WD (Set target where csv files are located)
setwd("D:/R Working Directory/Predict412/teamproject")

## This data set replaces the original 9822 and includes the selected subset vars including all interaction variables
ticdata = read.csv("ticdata_subset.csv", header= TRUE)
ticdata = ticdata[,-1]
ticdata[1:5,1:5]
dim(ticdata) # confirm 9822 x 157
class(ticdata) # confirm df, if not transform with ticdata = as.data.frame(ticdata)

### a.  Split 5822 and 4000 as Train and Test sets
test.set = ticdata[5823:nrow(ticdata),]
nrow(test.set) # confirm 4000
tic <- ticdata[1:5822,] # Select training data only
head(tic)
rows <- nrow(tic)
rows
compcases <- sum(complete.cases(tic))
# if this logical test returns false, data is missing.
rows==compcases
# there is no missing data

### b.  Split 5822 70/30 Train/Validate stratified on CARAVAN
summary(tic[,1])
# there are 348 positive responses in the total set
posinst <- nrow(tic[tic$CARAVAN=="insurance",])
posinst
# base rate for positive response (CARAVAN=1) is ~ 6%
baserate <- round(posinst/nrow(tic), digits=4)
baserate
# for 70/30 split, the test set should have 104 positive responses, +/- 1
pos_test_obs <- trunc(baserate*nrow(tic)*.3)
pos_test_obs

# partition into train and test
# stratify first, otherwise could be easy for one set to get no pos responses
# calculate how to stratify to get equal representation in train and test:
# how many positive instances should the train set have?
postrain <- trunc(posinst*.7)
postrain
# how many negative instances should the train set have?
negtrain <- trunc((rows-posinst)*.7)
negtrain
# add a stratum column which will be used to split the set into train and test:
strata <- strata(tic, stratanames=c("CARAVAN"), size=c(negtrain,postrain), method="srswor")
# split into train and test
tic.train <- tic[rownames(tic) %in% strata$ID_unit,]
tic.test <- tic[!rownames(tic) %in% strata$ID_unit,]
# here is how the respective sets look:
table(tic.train$CARAVAN)
table(tic.test$CARAVAN)
table(tic.train$CARAVAN)[1]/sum(table(tic.train$CARAVAN)[1:2])
table(tic.test$CARAVAN)[1]/sum(table(tic.test$CARAVAN)[1:2])
# after stratification, the test set is 2 rows longer than the strict 70% calc, and has 105 positive responses rather than the raw calc of 104, which is fine:
trainsize <- nrow(tic.train)
testsize <- nrow(tic.test)
trainsize
testsize

# Many algorithms won't do well if the data is presented one class then the other in the train set
tic.train <- tic.train [sample(nrow(tic.train)),]
# check = data.frame(names(train.bal.total), names(tic.test))
# nrow(tic.train)

# break out response and predictors for those methods that need them separated
ytrain <- ifelse(tic.train$CARAVAN=="insurance",1,0)
which(colnames(tic.train)=="CARAVAN")
xtrain <- as.data.frame(tic.train[,-1])
ytest <- ifelse(tic.test$CARAVAN=="insurance",1,0)
xtest <- as.data.frame(tic.test[,-1])
# save number of true positive responses in the test set for use in table later
val_truepos <- sum(ytest)
# make a formula for general use across methods
ticvars <- setdiff(colnames(tic.train),list('CARAVAN'))
#ticvars
# ticformula <- as.formula(paste('CARAVAN=="CARAVAN"', paste(ticvars,collapse=' + '),sep=' ~ '))
ticformula <- as.formula(paste('CARAVAN', paste(ticvars,collapse=' + '),sep=' ~ '))
ticformula

# set up test data
ytestfull <- test.set$CARAVAN
head(ytestfull)
ytestfull_binary <- ifelse(ytestfull=="noinsurance", 0,1)
test_truepos <- sum(ytestfull_binary)
head(ytestfull_binary)

which(colnames(test.set)=="CARAVAN")
xtestfull <- test.set[, -1]

### c. Scoring
# set up repository to collect results
trainpreds <- as.data.frame(ytrain)
testpreds <- as.data.frame(ytest)
colnames(trainpreds) <- "trainy"
colnames(testpreds) <- "testy"

# set up metrics frame - run once
combinedresults <- as.data.frame(c(1:8))
combinedresults <- t(combinedresults)[-1,]
colnames(combinedresults) <- c("Data_Set", "Method_Name", "Train_Set_Size", "Test_Set_True_Positives", "TP_Identified", "Baseline", "Lift", "Balanced_Train_Set")
combinedresults

# CoIL asked for the 800 test observations with highest probability of being positive responses
# The final holdout set has 4000 rows, so 800 is the top 20%
# We want to do the same thing in our test set (to identify our best model)
# our test set has 1748 rows
valsetsize <- nrow(tic.test)
valsetsize
# we want to select our top 20%, so this is 350 predictions:
topquintile_val <- round((valsetsize*.2), digits=0)
topquintile_val
# In any random sample of 350 observations from the data, there should be 20 or 21 positive responses:
baserate*topquintile_val
# this is very close to 21, so let's round it up
Baseline_validation <- 21
# the best model is the one that achieves the highest lift over 21 in the top 350
# (350 with highest predicted Pr(yhat=1|X))

# do the same thing for test data
testsetsize <- nrow(test.set)
testsetsize
# we want to select our top 20%, so this is 800 predictions:
topquintile_test <- round((testsetsize*.2), digits=0)
topquintile_test
# In any random sample of 800 observations from the data, there should be 47 or 48 positive responses:
baserate*topquintile_test
# round
Baseline_test <- round(baserate*topquintile_test, digits=0)
Baseline_test
# the best model is the one that achieves the highest lift over 48 in the top 800
# (800 with highest predicted Pr(yhat=1|X))

# create functions to evaluate results
evaluate_validation <- function(dataset, i, methodname, predictions, combinedresults){
  lift <- as.data.frame(cbind(ytest, predictions))
  colnames(lift) <- c("y", "yhat")
  order <- lift[order(lift$yhat,decreasing=TRUE),]
  liftcut <- order[1:350,]
  TP <- sum(liftcut$y)
  Lift <- TP-Baseline_validation
  results <- as.vector(c(dataset, methodname, i, val_truepos, TP, Baseline_validation, Lift, BT))
}

evaluate_test <- function(dataset, i, methodname, predictions, combinedresults){
  lift <- as.data.frame(cbind(ytestfull_binary, predictions))
  colnames(lift) <- c("y", "yhat")
  order <- lift[order(lift$yhat,decreasing=TRUE),]
  liftcut <- order[1:800,]
  TP <- sum(liftcut$y)
  Lift <- TP-Baseline_test
  results <- as.vector(c(dataset, methodname, i, test_truepos, TP, Baseline_test, Lift, BT))
}

evaluate_validation_learningcurve <- function(dataset, i, methodname, predictions, combinedresults){
  lift <- as.data.frame(cbind(ytest[1:i], predictions))
  colnames(lift) <- c("y", "yhat")
  order <- lift[order(lift$yhat,decreasing=TRUE),]
  liftcut <- order[1:350,]
  TP <- as.numeric(sum(liftcut$y))
  Lift <- as.numeric(TP-Baseline_validation)
  results <- as.vector(c(dataset, methodname, i, val_truepos, TP, Baseline_validation, Lift, BT))
}

### D.  Balance the Train set on CARAVAN 'insurance' class 
# balance train data
library(unbalanced)
set.seed(123)
### couldn't run the library on the plane, so tried without ####
#balance the dataset
# 'type' parameter: The argument type can take the following values: "ubOver" (over-sampling), "ubUnder" (undersampling), "ubSMOTE" (SMOTE), "ubOSS" (One Side Selection), "ubCNN" (Condensed Nearest Neighbor), "ubENN" (Edited Nearest Neighbor), "ubNCL" (Neighborhood Cleaning Rule),"ubTomek" (Tomek Link)
ytrainfactor <- as.factor(ytrain)

balancedData<- ubBalance(as.data.frame(xtrain),ytrainfactor, type="ubOver")
bal.tic.train<-cbind(balancedData$X,balancedData$Y)
head(bal.tic.train)
bal.tic.train <- rename(bal.tic.train, c("balancedData$Y" = "CARAVAN"))
bal.xtrain <- as.data.frame(balancedData$X)
head(bal.xtrain)
bal.ytrain <- bal.tic.train$CARAVAN
summary(bal.tic.train)
bal.ytrain <- bal.tic.train$CARAVAN
bal.tic.train2 = bal.tic.train
bal.tic.train2$CARAVAN[1:5]
bal.tic.train2$CARAVAN = factor(bal.tic.train2$CARAVAN, labels=c("noinsurance","insurance"))
bal.tic.train2$CARAVAN[1:5]
str(bal.tic.train2[,"CARAVAN"])
class(bal.tic.train2)
names(bal.tic.train2)
nrow(bal.tic.train2)
count(bal.tic.train$CARAVAN)
count(tic.train$CARAVAN)
count(tic.test$CARAVAN)
count(ytrain)
##################################################################################
# deal with rank deficiency
linearcomb <- findLinearCombos(bal.tic.train)
remove <- linearcomb$remove
bal.tic.train2 <- bal.tic.train[, -remove]
findLinearCombos(bal.tic.train2)
# recast ticformula
ticvars <- setdiff(colnames(bal.tic.train2),list('CARAVAN'))
ticformula2 <- as.formula(paste('CARAVAN', paste(ticvars,collapse=' + '),sep=' ~ '))
ticformula2
##################################################################################
# 2) Models
##################################################################################
###################################################################################
# logistic regression via glm
# next three lines pass the method name, whether balanced data was used, and the data set
# to the final results table:

# train/validate
methodname <- "logistic regression via glm (R) w/Int"
BT <- "Y"
dataset <- "Train/Validate"
i <- trainsize
tic.glm <- glm(ticformula2, data=bal.tic.train2, family=binomial)
predictions <- predict(tic.glm, newdata=tic.test, type='response')
results1 <- evaluate_validation(dataset, i, methodname, predictions, combinedresults)
combinedresults <- rbind(combinedresults, results1)
combinedresults

# test convergence
tic.glm$converged

# save the resulting model
model <- tic.glm$model
#write.xlsx2(model, "model.xlsx")

# test
methodname <- "logistic regression via glm (R) w/Int"
BT <- "Y"
dataset <- "Test"
predictions <- predict(tic.glm, newdata=xtestfull, type='response')
results1 <- evaluate_test(dataset, i, methodname, predictions, combinedresults)
combinedresults <- rbind(combinedresults, results1)
combinedresults

write.xlsx2(combinedresults, "combinedresults.xlsx", row.names=FALSE, col.names=TRUE)
############################
# modified learning curve
library(ggplot2)

# set up vector of train set sizes
trainfraction  <- seq(0.2,1,0.1)
trainsize <- nrow(bal.tic.train2)
trainlength <- trunc(as.vector(trainsize*trainfraction))
trainlength


#iterate model over increasing sample size
for(i in trainlength){  
 methodname <- "logistic regression via glm (R) w/Int"
    BT <- "Y"
    dataset <- "Train/Validate"
    tic.glm <- glm(ticformula2, data=bal.tic.train2[1:i,], family=binomial)
    predictions <- predict(tic.glm, newdata=tic.test[1:i,], type="response")
    
  # train results 
    results1 <- evaluate_validation_learningcurve(dataset, i, methodname, predictions, combinedresults)
    combinedresults <- rbind(combinedresults, results1)
 
  # test results
    methodname <- "logistic regression via glm (R) w/Int"
    BT <- "Y"
    dataset <- "Test"
    predictions <- predict(tic.glm, newdata=xtestfull, type='response')
    results1 <- evaluate_test(dataset, i, methodname, predictions, combinedresults)
    combinedresults <- rbind(combinedresults, results1)
 }
rownames(combinedresults) <- NULL
str(combinedresults)
write.xlsx2(combinedresults, "combinedresults.xlsx", row.names=FALSE, col.names=TRUE)

# break out train and test results
combinedresults <- as.data.frame(combinedresults)
trainresults <- combinedresults[combinedresults$Data_Set=="Train/Validate", ]
testresults <- combinedresults[combinedresults$Data_Set=="Test", ]
cols <- c(2,3,5)
totalresults <- cbind(trainresults[, cols], testresults$TP_Identified)
colnames(totalresults) <- c("Method", "TrainSetSize", "TrainTP", "TestTP")
rownames(totalresults) <- NULL
TrainTP <- as.numeric(as.vector(totalresults$TrainTP))
TestTP <- as.numeric(as.vector(totalresults$TestTP))
totalresults

# get the range for the x and y axis
xrange <- range(trainlength)
yrange <- range(c(0,max(TestTP)))
linetype <- c(1:2)
plotchar <- seq(18,19,1)

#plot the learning curve
plot(xrange,yrange,xlab="Number of training examples",ylab="True Positives Identified", main="Modified Learning Curve - Total True Positives Identified by Train Set Size", cex.main=0.9)
lines(trainlength, TrainTP, type="b", lwd=1.5, lty=linetype[1], col="blue", pch=plotchar[1]) 
lines(trainlength, TestTP, type="b", lwd=1.5, lty=linetype[2], col="gray", pch=plotchar[2]) 
legend(x=2000, y=20, c("train", "test"), cex=0.8, col=c("blue", "gray"), pch=plotchar, lty=linetype, title="GLM Modified Learning Curve")

######################################################################################
# work the same concept, but showing percent of possible true positives actually identified
TrainPctID  <- as.vector(TrainTP/105)
TestPctID <- as.vector(TestTP/238)
max(TrainPctID)
max(TestPctID)
# get the range for the x and y axis
xrange <- range(trainlength)
yrange <- range(c(0,max(c(max(TrainPctID), max(TestPctID)))))
linetype <- c(1:2)
plotchar <- seq(18,19,1)

#plot the learning curve
plot(xrange,yrange,xlab="Number of training examples",ylab="True Positives Identified", main="Modified Learning Curve:  Percentage of Total True Positives Identified by Train Set Size", cex.main=0.9)
lines(trainlength, TrainPctID, type="b", lwd=1.5, lty=linetype[1], col="blue", pch=plotchar[1]) 
lines(trainlength, TestPctID, type="b", lwd=1.5, lty=linetype[2], col="gray", pch=plotchar[2]) 
legend(x=2000, y=0.2, c("train", "test"), cex=0.8, col=c("blue", "gray"), pch=plotchar, lty=linetype, title="GLM Modified Learning Curve")
######################################################################################
# naive bayes
library(e1071)
# train/validate
BT <- "Y"
methodname <- "naive bayes w/Int"
dataset <- "Train/Validate"
tic.nb <- naiveBayes(bal.xtrain, bal.ytrain)
predictions <- predict(tic.nb, newdata=xtest, type='raw')[,2]
head(predictions)
results1 <- evaluate_validation(dataset, methodname, predictions=predictions, combinedresults)
combinedresults <- rbind(combinedresults, results1)
combinedresults

# on unbalanced data:
BT <- "N"
methodname <- "naive bayes w/Int"
dataset <- "Train/Validate"
tic.nb <- naiveBayes(xtrain, ytrain)
predictions <- predict(tic.nb, newdata=xtest, type='raw')[,2]
head(predictions)
results1 <- evaluate_validation(dataset, methodname, predictions=predictions, combinedresults)
combinedresults <- rbind(combinedresults, results1)
combinedresults

# test
methodname <- "naive bayes w/Int"
BT <- "N"
dataset <- "Test"
predictions <- predict(tic.nb, newdata=xtestfull, type='raw')[,2]
results1 <- evaluate_test(dataset, methodname, predictions, combinedresults)
combinedresults <- rbind(combinedresults, results1)
combinedresults
write.xlsx2(combinedresults, "combinedresults.xlsx", row.names=FALSE, col.names=TRUE)
